---
---

@string{aps = {American Physical Society,}}

@misc{scalena2025steeringlargelanguagemodels,
    title={Steering Large Language Models for Machine Translation Personalization}, 
    author={Daniel Scalena* and Gabriele Sarti* and Arianna Bisazza and Elisabetta Fersini and Malvina Nissim},
    year={2025},
    eprint={2505.16612},
    archivePrefix={arXiv},
    primaryClass={cs.CL},
    url={https://arxiv.org/abs/2505.16612},
    abstract = {High-quality machine translation systems based on large language models (LLMs) have simplified the production of personalized translations reflecting specific stylistic constraints. However, these systems still struggle in settings where stylistic requirements are less explicit and might be harder to convey via prompting. We explore various strategies for personalizing LLM-generated translations in low-resource settings, focusing on the challenging literary translation domain. We explore prompting strategies and inference-time interventions for steering model generations towards a personalized style, and propose a contrastive framework exploiting latent concepts extracted from sparse autoencoders to identify salient personalization properties. Our results show that steering achieves strong personalization while preserving translation quality. We further examine the impact of steering on LLM representations, finding model layers with a relevant impact for personalization are impacted similarly by multi-shot prompting and our steering method, suggesting similar mechanism at play.},
    selected={true},
    html={https://arxiv.org/abs/2505.16612},
    preview={personalizedMT.png}
}


@inproceedings{scalena-etal-2024-gentle,
    title = "A Gentle Push Funziona Benissimo: Making Instructed Models in {I}talian via Contrastive Activation Steering",
    author = "Scalena, Daniel  and
      Fersini, Elisabetta  and
      Nissim, Malvina",
    editor = "Dell'Orletta, Felice  and
      Lenci, Alessandro  and
      Montemagni, Simonetta  and
      Sprugnoli, Rachele",
    booktitle = "Proceedings of the 10th Italian Conference on Computational Linguistics (CLiC-it 2024)",
    month = dec,
    year = "2024",
    address = "Pisa, Italy",
    publisher = "CEUR Workshop Proceedings",
    url = "https://aclanthology.org/2024.clicit-1.98/",
    pages = "909--920",
    ISBN = "979-12-210-7060-6",
    abstract = "Adapting models to a language that was only partially present in the pre-training data requires fine-tuning, which is expensive in terms of both data and computational resources. As an alternative to fine-tuning, we explore the potential of activation steering-based techniques to enhance model performance on Italian tasks. Through our experiments we show that Italian steering (i) can be successfully applied to different models, (ii) achieves performances comparable to, or even better than, fine-tuned models for Italian, and (iii) yields higher quality and consistency in Italian generations. We also discuss the utility of steering and fine-tuning in the contemporary LLM landscape where models are anyway getting high Italian performances even if not explicitly trained in this language.",
    pdf = "Italian_Steering_CLiC_it_2024.pdf",
    selected={true},
    preview={}
}

@inproceedings{scalena-etal-2024-multi,
    title = "Multi-property Steering of Large Language Models with Dynamic Activation Composition",
    author = "Scalena, Daniel  and
      Sarti, Gabriele  and
      Nissim, Malvina",
    editor = "Belinkov, Yonatan  and
      Kim, Najoung  and
      Jumelet, Jaap  and
      Mohebbi, Hosein  and
      Mueller, Aaron  and
      Chen, Hanjie",
    booktitle = "Proceedings of the 7th BlackboxNLP Workshop: Analyzing and Interpreting Neural Networks for NLP",
    month = nov,
    year = "2024",
    address = "Miami, Florida, US",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2024.blackboxnlp-1.34",
    pages = "577--603",
    abstract = "Activation steering methods were shown to be effective in conditioning language model generation by additively intervening over models{'} intermediate representations. However, the evaluation of these techniques has so far been limited to single conditioning properties and synthetic settings. In this work, we conduct a comprehensive evaluation of various activation steering strategies, highlighting the property-dependent nature of optimal parameters to ensure a robust effect throughout generation. To address this issue, we propose Dynamic Activation Composition, an information-theoretic approach to modulate the steering intensity of one or more properties throughout generation. Our experiments on multi-property steering show that our method successfully maintains high conditioning while minimizing the impact of conditioning on generation fluency.",
    pdf = "https://aclanthology.org/2024.blackboxnlp-1.34.pdf",
    html={https://aclanthology.org/2024.blackboxnlp-1.34/}, 
    selected={true},
    preview={duckSteering.jpeg}
}

@misc{scalena2023let,
      title={Let the Models Respond: Interpreting Language Model Detoxification Through the Lens of Prompt Dependence}, 
      author={Daniel Scalena and Gabriele Sarti and Malvina Nissim and Elisabetta Fersini},
      year={2023},
      eprint={2309.00751},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      abstract = "Due to language models' propensity to generate toxic or hateful responses, several techniques were developed to align model generations with users' preferences. Despite the effectiveness of such methods in improving the safety of model interactions, their impact on models' internal processes is still poorly understood. In this work, we apply popular detoxification approaches to several language models and quantify their impact on the resulting models' prompt dependence using feature attribution methods. We evaluate the effectiveness of counter-narrative fine-tuning and compare it with reinforcement learning-driven detoxification, observing differences in prompt reliance between the two methods despite their similar detoxification performances.",
      pdf = "scalena-etal-letmodel.pdf",
      html={https://arxiv.org/abs/2309.00751},
      preview={LetTheModelRespond.png}
}


@inproceedings{rizzi-etal-2023-mind,
    title = "{MIND} at {S}em{E}val-2023 Task 11: From Uncertain Predictions to Subjective Disagreement",
    author = "Rizzi, Giulia  and
      Astorino, Alessandro  and
      Scalena, Daniel  and
      Rosso, Paolo  and
      Fersini, Elisabetta",
    booktitle = "Proceedings of the The 17th International Workshop on Semantic Evaluation (SemEval-2023)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.semeval-1.77",
    html = {https://aclanthology.org/2023.semeval-1.77},
    pages = "556--564",
    abstract = "This paper describes the participation of the research laboratory MIND, at the University of Milano-Bicocca, in the SemEval 2023 task related to Learning With Disagreements (Le-Wi-Di). The main goal is to identify the level of agreement/disagreement from a collection of textual datasets with different characteristics in terms of style, language and task.The proposed approach is grounded on the hypothesis that the disagreement between annotators could be grasped by the uncertainty that a model, based on several linguistic characteristics, could have on the prediction of a given gold label.",
    pdf = "rizzi-etal-2023-mind.pdf",
    selected={false},
    preview={mind_logo.png}
}




@mastersthesis{Scalena-BsThesis,
    author = {Scalena, Daniel},
    institution = {University of Milano - Bicocca},
    title = {Tecniche di Natural Language Processing per il riconoscimento dei discorsi d'odio sui social network},
    year = "2021",
    month = {jul},
    html = {https://github.com/DanielSc4/Bachelor-s-thesis}
}

