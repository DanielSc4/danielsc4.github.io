---
---

@string{aps = {American Physical Society,}}


@misc{scalena2024italianSteering,
      title={A gentle push funziona benissimo: making instructed models in Italian via contrastive activation steering}, 
      author={Daniel Scalena and Elisabetta Fersini and Malvina Nissim},
      year={2024},
      abstract = "Adapting models to a language that was only partially present in the pre-training data requires fine-tuning, which is expensive in terms of both data and computational resources. As an alternative to fine-tuning, we explore the potential of activation steering-based techniques to enhance model performance on Italian tasks. Through our experiments we show that Italian steering (i) can be successfully applied to different models, (ii) achieves performances comparable to, or even better than, fine-tuned models for Italian, and (iii) yields higher quality and consistency in Italian generations. We also discuss the utility of steering and fine-tuning in the contemporary LLM landscape where models are anyway getting high Italian performances even if not explicitly trained in this language.",
      pdf = "Italian_Steering_CLiC_it_2024.pdf",
      html={}, 
      selected={true},
      preview={}
}

@misc{scalena2024multipropertysteeringlargelanguage,
      title={Multi-property Steering of Large Language Models with Dynamic Activation Composition}, 
      author={Daniel Scalena and Gabriele Sarti and Malvina Nissim},
      year={2024},
      eprint={2406.17563},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      abstract = "Activation steering methods were shown to be effective in conditioning language model generation by additively intervening over models' intermediate representations. However, the evaluation of these techniques has so far been limited to single conditioning properties and synthetic settings. In this work, we conduct a comprehensive evaluation of various activation steering strategies, highlighting the property-dependent nature of optimal parameters to ensure a robust effect throughout generation. To address this issue, we propose Dynamic Activation Composition, an information-theoretic approach to modulate the steering intensity of one or more properties throughout generation. Our experiments on multi-property steering show that our method successfully maintains high conditioning while minimizing the impact of conditioning on generation fluency.",
      pdf = "DAC_paper.pdf",
      html={https://arxiv.org/abs/2406.17563}, 
      selected={true},
      preview={duckSteering.jpeg}
}

@misc{scalena2023let,
      title={Let the Models Respond: Interpreting Language Model Detoxification Through the Lens of Prompt Dependence}, 
      author={Daniel Scalena and Gabriele Sarti and Malvina Nissim and Elisabetta Fersini},
      year={2023},
      eprint={2309.00751},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      abstract = "Due to language models' propensity to generate toxic or hateful responses, several techniques were developed to align model generations with users' preferences. Despite the effectiveness of such methods in improving the safety of model interactions, their impact on models' internal processes is still poorly understood. In this work, we apply popular detoxification approaches to several language models and quantify their impact on the resulting models' prompt dependence using feature attribution methods. We evaluate the effectiveness of counter-narrative fine-tuning and compare it with reinforcement learning-driven detoxification, observing differences in prompt reliance between the two methods despite their similar detoxification performances.",
      pdf = "scalena-etal-letmodel.pdf",
      html={https://arxiv.org/abs/2309.00751},
      selected={true},
      preview={LetTheModelRespond.png}
}


@inproceedings{rizzi-etal-2023-mind,
    title = "{MIND} at {S}em{E}val-2023 Task 11: From Uncertain Predictions to Subjective Disagreement",
    author = "Rizzi, Giulia  and
      Astorino, Alessandro  and
      Scalena, Daniel  and
      Rosso, Paolo  and
      Fersini, Elisabetta",
    booktitle = "Proceedings of the The 17th International Workshop on Semantic Evaluation (SemEval-2023)",
    month = jul,
    year = "2023",
    address = "Toronto, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2023.semeval-1.77",
    html = {https://aclanthology.org/2023.semeval-1.77},
    pages = "556--564",
    abstract = "This paper describes the participation of the research laboratory MIND, at the University of Milano-Bicocca, in the SemEval 2023 task related to Learning With Disagreements (Le-Wi-Di). The main goal is to identify the level of agreement/disagreement from a collection of textual datasets with different characteristics in terms of style, language and task.The proposed approach is grounded on the hypothesis that the disagreement between annotators could be grasped by the uncertainty that a model, based on several linguistic characteristics, could have on the prediction of a given gold label.",
    pdf = "rizzi-etal-2023-mind.pdf",
    selected={false},
    preview={mind_logo.png}
}




@mastersthesis{Scalena-BsThesis,
    author = {Scalena, Daniel},
    institution = {University of Milano - Bicocca},
    title = {Tecniche di Natural Language Processing per il riconoscimento dei discorsi d'odio sui social network},
    year = "2021",
    month = {jul},
    html = {https://github.com/DanielSc4/Bachelor-s-thesis}
}

