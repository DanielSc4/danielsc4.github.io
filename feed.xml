<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.2">Jekyll</generator><link href="https://danielsc4.github.io/feed.xml" rel="self" type="application/atom+xml" /><link href="https://danielsc4.github.io/" rel="alternate" type="text/html" hreflang="en" /><updated>2023-07-17T15:48:39+00:00</updated><id>https://danielsc4.github.io/feed.xml</id><title type="html">blank</title><subtitle>Master CS student
</subtitle><entry><title type="html">Let the Models Respond: interpreting the detoxification process of LMs</title><link href="https://danielsc4.github.io/blog/2023/interpreting-detox-LM/" rel="alternate" type="text/html" title="Let the Models Respond: interpreting the detoxification process of LMs" /><published>2023-07-12T15:59:00+00:00</published><updated>2023-07-12T15:59:00+00:00</updated><id>https://danielsc4.github.io/blog/2023/interpreting-detox-LM</id><content type="html" xml:base="https://danielsc4.github.io/blog/2023/interpreting-detox-LM/"><![CDATA[<h4 id="framing">Framing</h4>

<p>üö® This blogpost contains examples which are offensive in nature.</p>

<p>This research project was carried out by <a href="https://www.danielsc4.it/">me üëãüèº</a> during the internship period at the <a href="https://www.rug.nl/research/clcg/research/cl/?lang=en">Computational Linguistics Research Lab</a> at the <a href="https://www.rug.nl/">University of Groningen</a>. Currently, the work is still in progress and nearing completion. The results and status of the work do not represent the final state of the research.</p>

<p>The work is supervised by:</p>
<ul>
  <li><a href="https://gsarti.com/">Gabriele Sarti</a>, PhD student @ University of Groningen</li>
  <li><a href="https://www.rug.nl/staff/m.nissim/">Malvina Nissim</a>, Full professor @ University of Groningen</li>
  <li><a href="https://en.unimib.it/elisabetta-fersini">Elisabetta Fersini</a>, Associate professor @ University of Milano - Bicocca</li>
</ul>

<h2 id="abstract">Abstract</h2>

<p><strong>Language Models</strong> (LMs) represent complex systems that are difficult to manage and deploy safely. For this reason, various techniques have been proposed over time with the aim of detoxifying and controlling the behaviour of the models after their training process. With this in mind, this research project aims to <strong>explore the potential of the model detoxification process</strong>. Known techniques of <em>fine-tuning</em> and <em>Reinforcement Learning from Human Feedback</em> (RLHF) will be explored leading to less toxic models. The work also aims to <strong>understand the detoxification process through an exploration on the interpretability of the models</strong> themselves, having the ultimate goal of <strong>not limiting their responses</strong> but offering a contronarrative with respect to potentially toxic prompts.</p>

<h2 id="introduction-and-state-of-the-art">Introduction and State Of The Art</h2>
<p>In the recent period, LMs are observing a rise in terms of parameters, complexity and consequently results obtained that, in some cases, manage to exceed even human capabilities for specific tasks <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf">(Radford and Narasimhan, 2018)</a>. All this power, however, comes from large amounts of data used in the pre-training phase of LMs that learn primarily from corpora extracted from the Internet, forums and social media. The large availability of text on these platforms certainly implies an ease in extracting various aspects of language useful for the learning process but brings with it issues especially relevant to the quality and content itself in the text. Indeed, it is not at all uncommon to find toxic, dangerous, privacy-compromising content or more complex phenomena such as unintended bias hidden in the text itself <a href="https://dl.acm.org/doi/10.1145/3442188.3445922">(Bender et al., 2021)</a>. All these aspects, which are difficult to control <em>a priori</em>, inevitably end up in the data that make up the LMs‚Äô pre-training datasets, leading them to language generations that cannot always be considered safe and harmless <a href="https://aclanthology.org/2020.findings-emnlp.301.pdf">(Gehman et al., 2020)</a>.</p>

<p>It is for this reason that efforts in research have been made to try to mitigate these phenomena as much as possible, both from the data point of view and from the point of view of the pre-trained LMs. Among the best known techniques can be found fine-tuning, RLHF <a href="https://arxiv.org/abs/2204.05862">(Bai et al., 2022)</a> and model steering <a href="https://arxiv.org/abs/1912.02164">(Dathathri et al. 2020)</a>. These techniques turn out to be more than effective in controlling the toxicity in model input/output but, especially in the presence of particularly ‚Äútendentious‚Äù cases it still remains possible to fool the models that still end up generating potentially toxic or unsafe responses. In addition, the most well-known response pattern to prompts deemed as dangerous is to stop the conversation, trying to stop proceeding to toxic behaviors (e.g., ‚ÄúAs an AI Language Model I cannot answer this question, ‚Ä¶‚Äù).</p>

<p><img src="https://www.danielsc4.it/assets/img/detox_LMs/Example_chatGPT_toxic.png" alt="Toxic Prompt on ChatGPT that generates conversation blocking" title="Title" />
<em>Toxic Prompt on <a href="https://openai.com/blog/chatgpt">ChatGPT</a> that generates conversation blocking</em></p>

<p>With the following research project, we therefore want to investigate the detoxification process, pushing not only the models to be safer but exploring their potential by allowing them to respond even to potentially toxic prompts by offering a useful counter narrative to send the conversation forward to reason with the user who authored the original prompt.</p>

<p>As can be guessed, it is imperative that such a process be as transparent as possible. For this very reason, techniques for interpreting the models themselves will be employed to discover how the models change their generation. This will hopefully lead to discovering not only new features of the models but also what techniques might be most effective for the safety and effectiveness of the LMs themselves.</p>

<h2 id="approach">Approach</h2>

<h2 id="experiment-and-results">Experiment and results</h2>

<h3 id="dataset">Dataset</h3>
<h3 id="baseline">Baseline</h3>
<h3 id="evaluation-setup">Evaluation setup</h3>
<h3 id="result">Result</h3>

<h2 id="current-status-and-new-research-questions">Current status and new research questions</h2>

<h2 id="extras">Extras</h2>

<p>Table:</p>

<table>
  <thead>
    <tr>
      <th style="text-align: left">Left aligned</th>
      <th style="text-align: center">Center aligned</th>
      <th style="text-align: right">Right aligned</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td style="text-align: left">Left 1</td>
      <td style="text-align: center">center 1</td>
      <td style="text-align: right">right 1</td>
    </tr>
    <tr>
      <td style="text-align: left">Left 2</td>
      <td style="text-align: center">center 2</td>
      <td style="text-align: right">right 2</td>
    </tr>
    <tr>
      <td style="text-align: left">Left 3</td>
      <td style="text-align: center">center 3</td>
      <td style="text-align: right">right 3</td>
    </tr>
  </tbody>
</table>

<p>In line math equation: \(E = mc^2\).</p>

<p>Non in line math equation:</p>

\[\sum_{k=1}^\infty |\langle x, e_k \rangle|^2 \leq \|x\|^2\]

<p>Code:</p>
<div class="language-c++ highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kt">int</span> <span class="nf">main</span><span class="p">(</span><span class="kt">int</span> <span class="n">argc</span><span class="p">,</span> <span class="kt">char</span> <span class="k">const</span> <span class="err">\</span><span class="o">*</span><span class="n">argv</span><span class="p">[])</span>
<span class="p">{</span>
    <span class="n">string</span> <span class="n">myString</span><span class="p">;</span>

    <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="s">"input a string: "</span><span class="p">;</span>
    <span class="n">getline</span><span class="p">(</span><span class="n">cin</span><span class="p">,</span> <span class="n">myString</span><span class="p">);</span>
    <span class="kt">int</span> <span class="n">length</span> <span class="o">=</span> <span class="n">myString</span><span class="p">.</span><span class="n">length</span><span class="p">();</span>

    <span class="kt">char</span> <span class="n">charArray</span> <span class="o">=</span> <span class="k">new</span> <span class="kt">char</span> <span class="o">*</span> <span class="p">[</span><span class="n">length</span><span class="p">];</span>

    <span class="n">charArray</span> <span class="o">=</span> <span class="n">myString</span><span class="p">;</span>
    <span class="k">for</span><span class="p">(</span><span class="kt">int</span> <span class="n">i</span> <span class="o">=</span> <span class="mi">0</span><span class="p">;</span> <span class="n">i</span> <span class="o">&lt;</span> <span class="n">length</span><span class="p">;</span> <span class="o">++</span><span class="n">i</span><span class="p">){</span>
        <span class="n">cout</span> <span class="o">&lt;&lt;</span> <span class="n">charArray</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">&lt;&lt;</span> <span class="s">" "</span><span class="p">;</span>
    <span class="p">}</span>

    <span class="k">return</span> <span class="mi">0</span><span class="p">;</span>
<span class="p">}</span>
</code></pre></div></div>]]></content><author><name></name></author><category term="journal" /><summary type="html"><![CDATA[Journal to keep track of work during internship @RUG]]></summary></entry></feed>