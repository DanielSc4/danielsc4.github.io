<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Let the Models Respond: Interpreting the Detoxification process of LMs | Daniel Scalena </title> <meta name="author" content="Daniel Scalena"> <meta name="description" content="Journal to keep track of work during internship @RUG"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/favicon.png?be077859a19aab29ce63dbd9d404366d"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://danielsc4.it/blog/2023/interpreting-detox-LM/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Daniel</span> Scalena </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">Let the Models Respond: Interpreting the Detoxification process of LMs</h1> <p class="post-meta"> Created in July 12, 2023 </p> <p class="post-tags"> <a href="/blog/2023"> <i class="fa-solid fa-calendar fa-sm"></i> 2023 </a>   ·   <a href="/blog/category/journal"> <i class="fa-solid fa-tag fa-sm"></i> journal</a> </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h4"><a href="#framing">Framing</a></li> <li class="toc-entry toc-h2"><a href="#-abstract">📜 Abstract</a></li> <li class="toc-entry toc-h2"> <a href="#-introduction-and-state-of-the-art">🎨 Introduction and State Of The Art</a> <ul> <li class="toc-entry toc-h4"><a href="#goals">Goals</a></li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#-approach">🦾 Approach</a> <ul> <li class="toc-entry toc-h4"><a href="#how-to-deal-with-large-lms">How to deal with Large LMs?</a></li> <li class="toc-entry toc-h4"><a href="#how-to-efficiently-train-quantized-large-lms">How to efficiently train quantized Large LMs?</a></li> <li class="toc-entry toc-h4"><a href="#letting-lms-respond-with-contronarrative">Letting LMs Respond with Contronarrative</a></li> <li class="toc-entry toc-h4"> <a href="#fine-tuning-and-reinforcement-learning-from-automatic-feedback">Fine-tuning and Reinforcement Learning from (Automatic) Feedback</a> <ul> <li class="toc-entry toc-h5"><a href="#toxicity-meter-an-easy-way-to-measure-lms-toxicity">Toxicity Meter: an easy way to measure LMs toxicity</a></li> </ul> </li> </ul> </li> <li class="toc-entry toc-h2"> <a href="#-experiment-and-results">🔬 Experiment and results</a> <ul> <li class="toc-entry toc-h3"><a href="#result">Result</a></li> </ul> </li> <li class="toc-entry toc-h2"><a href="#-current-status-and-new-research-questions">🚀 Current status and new research questions</a></li> </ul> </div> <hr> <div id="markdown-content"> <h4 id="framing"><strong>Framing</strong></h4> <p>🚨 This blogpost contains examples which are offensive in nature.</p> <p>This research project was carried out by <a href="https://www.danielsc4.it/" rel="external nofollow noopener" target="_blank">me 👋🏼</a> during the internship period at the <a href="https://www.rug.nl/research/clcg/research/cl/?lang=en" rel="external nofollow noopener" target="_blank">Computational Linguistics Research Lab</a> at the <a href="https://www.rug.nl/" rel="external nofollow noopener" target="_blank">University of Groningen</a>. Currently, the work is still in progress and nearing completion. The results and status of the work do not represent the final state of the research.</p> <p>The work is supervised by:</p> <ul> <li> <a href="https://gsarti.com/" rel="external nofollow noopener" target="_blank">Gabriele Sarti</a>, PhD student @ University of Groningen</li> <li> <a href="https://www.rug.nl/staff/m.nissim/" rel="external nofollow noopener" target="_blank">Malvina Nissim</a>, Full professor @ University of Groningen</li> <li> <a href="https://en.unimib.it/elisabetta-fersini" rel="external nofollow noopener" target="_blank">Elisabetta Fersini</a>, Associate professor @ University of Milano - Bicocca</li> </ul> <hr> <p><br></p> <h2 id="-abstract"><strong>📜 Abstract</strong></h2> <p><strong>Language Models</strong> (LMs) represent complex systems that are difficult to manage and deploy safely. For this reason, various techniques have been proposed over time with the aim of detoxifying and controlling the behaviour of the models after their training process. With this in mind, this research project aims to <strong>explore the potential of the model detoxification process</strong>. Known techniques of <em>fine-tuning</em> and <em>Reinforcement Learning from Human Feedback</em> (RLHF) will be explored leading to less toxic models. The work also aims to <strong>understand the detoxification process through an exploration on the interpretability of the models</strong> themselves, having the ultimate goal of <strong>not limiting their responses</strong> but offering a contronarrative with respect to potentially toxic prompts.</p> <p><br></p> <h2 id="-introduction-and-state-of-the-art"><strong>🎨 Introduction and State Of The Art</strong></h2> <p>In the recent period, LMs are observing a rise in terms of parameters, complexity and consequently results obtained that, in some cases, manage to exceed even human capabilities for specific tasks <a href="https://s3-us-west-2.amazonaws.com/openai-assets/research-covers/language-unsupervised/language_understanding_paper.pdf" rel="external nofollow noopener" target="_blank">(Radford and Narasimhan, 2018)</a>. All this power, however, comes from large amounts of data used in the pre-training phase of LMs that learn primarily from corpora extracted from the Internet, forums and social media. The large availability of text on these platforms certainly implies an ease in extracting various aspects of language useful for the learning process but brings with it issues especially relevant to the quality and content itself in the text. Indeed, it is not at all uncommon to find toxic, dangerous, privacy-compromising content or more complex phenomena such as unintended bias hidden in the text itself <a href="https://dl.acm.org/doi/10.1145/3442188.3445922" rel="external nofollow noopener" target="_blank">(Bender et al., 2021)</a>. All these aspects, which are difficult to control <em>a priori</em>, inevitably end up in the data that make up the LMs’ pre-training datasets, leading them to language generations that cannot always be considered safe and harmless <a href="https://aclanthology.org/2020.findings-emnlp.301.pdf" rel="external nofollow noopener" target="_blank">(Gehman et al., 2020)</a>.</p> <p>It is for this reason that efforts in research have been made to try to mitigate these phenomena as much as possible, both from the data point of view and from the point of view of the pre-trained LMs. Among the best known techniques can be found fine-tuning, RLHF <a href="https://arxiv.org/abs/2204.05862" rel="external nofollow noopener" target="_blank">(Bai et al., 2022)</a> and model steering <a href="https://arxiv.org/abs/1912.02164" rel="external nofollow noopener" target="_blank">(Dathathri et al. 2020)</a>. These techniques turn out to be more than effective in controlling the toxicity in model input/output but, especially in the presence of particularly “tendentious” cases it still remains possible to fool the models that still end up generating potentially toxic or unsafe responses. In addition, the most well-known response pattern to prompts deemed as dangerous is to stop the conversation, trying to stop proceeding to toxic behaviors (e.g., “As an AI Language Model I cannot answer this question, …”).</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/detox_LMs/Example_chatGPT_toxic.png" sizes="95vw"></source> <img src="/assets/img/detox_LMs/Example_chatGPT_toxic.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><em>Toxic Prompt on <a href="https://openai.com/blog/chatgpt" rel="external nofollow noopener" target="_blank">ChatGPT</a> that generates conversation blocking</em></p> <h4 id="goals">Goals</h4> <p>With the following research project, we therefore want to <strong>investigate the detoxification process</strong>, pushing not only the models to be safer but exploring their potential by <strong>allowing them to respond even to potentially toxic prompts</strong> by offering a useful counter narrative to send the conversation forward to reason with the user who authored the original prompt.</p> <p>As can be guessed, it is imperative that such <strong>a process be as transparent as possible</strong>. For this reason, techniques for interpreting the models themselves will be employed to discover how the models change their generation. This will hopefully lead to discovering not only new features of the models but also what techniques might be most effective for <strong>the safety and effectiveness of the LMs</strong> themselves.</p> <p><br></p> <h2 id="-approach"><strong>🦾 Approach</strong></h2> <p>Of the various techniques previously listed, fine-tuning and reinforcement learning represent the state of the art, also employed by industry for the most modern LMs. The main problem related to the use of these techniques, however, is the size of the models themselves. In fact, over the utlim years, there has been a trend toward growth in the number of parameters in language models, reaching and exceeding hundreds of billions in the case of the largest models (GPT-3/4, Bard, …). For these reasons, even just performing fine-tuning or applying reinforcement learning techniques seems to be quite impossible on consumer hardware or otherwise accessible to the research community. Even just maintaining a 7B model of parameters, on RAM or VRAM, would take more than 32GB.</p> <h4 id="how-to-deal-with-large-lms">How to deal with Large LMs?</h4> <p>However, there are several techniques that have emerged over time in the literature that aim to mitigate this type of issue. Indeed, it is possible to load models in Half Precision (16 bits instead of 32 bits) or, even more recently, in 8 bits and 4 bits through quantization techniques <a href="https://arxiv.org/abs/2208.07339" rel="external nofollow noopener" target="_blank">(Dettmers et al., 2022)</a>. These techniques allow dynamic mapping of tensors from the original 32bit model in Full Precision to 16bit tensors and, eventually in 8bit tensors, allowing a theoretical reduction of up to 400% (ideal case without training/inference data, in practice less given the necessary preservation of some parameters).</p> <p><em>Half precision input matrix \(X_{f16} \in \!R^{s×h}\), can be quantizited as follow</em>:</p> \[X_{i 8} = \biggl \lceil \frac{127 \cdot X_{f16}}{\max{(|{X_{f16}}_{i,j} |)}} \biggr \rfloor = \biggl \lceil \frac{127}{||X_{f16}||_{\infty}} \cdot X_{f16} \biggr \rfloor = \lceil {s_x}_{f16} X_{f_16} \rfloor\] <p><em>Scaling a tensor to his 8-bit version forces the range</em> \([-127, +127]\) <em>by multiplying with</em> \({s*x}*{f16}\) <em>which is 127 divided by the absolute maximum of the entire tensor. This is equivalent to dividing by the infinity norm and multiplying by 127. More info in the original paper.</em></p> <p>This advantage of matrix representation, however, comes at a cost in the inability to effectively modify the matrices within the model, in other words, to perform weight training.</p> <h4 id="how-to-efficiently-train-quantized-large-lms">How to efficiently train quantized Large LMs?</h4> <p>In order to fine-tune or otherwise modify the weights of the model there must be weights in FP32 or FP16 representation. For this very reason, <a href="https://arxiv.org/abs/2106.09685" rel="external nofollow noopener" target="_blank">(J. Hu et al., 2021)</a> with Low-Rank Adaptation (LoRA) aims to create adapters that, in parallel with the frozen weights of the model, allow one to circumvent the problem by offering trainable lower-rank matrices based on the frozen model. The details of this operation will not be exposed here (for more information look at the paper cited earlier) but it is important to mention how this solution allows not only the training of larger models but is shown to partially succeed in solving the catastrophic forgetting problem as well. The most convenient implementation, being integrated with 🤗 HuggingFace is the one provided by 🤗 <a href="https://huggingface.co/blog/peft" rel="external nofollow noopener" target="_blank">Peft</a>.</p> <h4 id="letting-lms-respond-with-contronarrative">Letting LMs Respond with Contronarrative</h4> <p>As previously mentioned, the state of the art so far has focused on generic detoxification of LMs, certainly leading them to be less toxic by avoiding responding to compromising prompts or otherwise imposing strong constraints on both the optimization process and the output of the model itself. In fact, based on what has been observed, the same models may be able to articulate more complex responses that capture even the most delicate aspects of the dialogue. Thus, we want precisely to explore this concept further, bringing, through fine-tuning, <strong>the model to a contronarrative generation responsive to the given prompt</strong>.</p> <p>For this purpose, <a href="https://aclanthology.org/2022.emnlp-main.549/" rel="external nofollow noopener" target="_blank">(Bonaldi et al., 2022)</a>, a dataset curated by experts is employed to provide accurate answers to prompts regarding topics and/or people particularly susceptible and vulnerable to online hate speech. The dataset, consists mainly of dialogues (thus with multiple prompt-response pairs); we chose to select each pair while maintaining all its antecedents, exploiting the potential of Chain-of-Thought <a href="https://arxiv.org/abs/2201.11903" rel="external nofollow noopener" target="_blank">(Wei et al., 2022)</a>.</p> <h4 id="fine-tuning-and-reinforcement-learning-from-automatic-feedback">Fine-tuning and Reinforcement Learning from (Automatic) Feedback</h4> <p>Fine-tuning and reinforcement learning of the models was employed using the <a href="https://github.com/DanielSc4/RewardLM" rel="external nofollow noopener" target="_blank">🥞 RewardLM</a> library. The library allows integration of the models with 🤗 HuggingFace (the <em>de facto</em> standard for OpenSource model sharing and manipulation), training and monitoring of the results obtained efficiently. In the case of Reinforcement Learning (<a href="https://github.com/DanielSc4/RewardLM#-reinforcement-learning-with-automatic-feedback-rlaf" rel="external nofollow noopener" target="_blank">RFAF</a>), besides all the hyperparameters involved, it is possible to specify different details of the reward model, being able to choose any classifier or a set more than one of them for greater efficiency.</p> <p>Particular attention can be paid to the Reinforcement Learning process where, following the diagram below, two identical initial models are kept in memory, one for reference and one that can be changed according to the direction imposed by the reward model.</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/detox_LMs/rlhf.png" sizes="95vw"></source> <img src="/assets/img/detox_LMs/rlhf.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><em>Training scheme for the RLAF algorithm implemented on <a href="https://github.com/DanielSc4/RewardLM" rel="external nofollow noopener" target="_blank">🥞 RewardLM</a>. Image <a href="https://huggingface.co/blog/rlhf" rel="external nofollow noopener" target="_blank">source</a>.</em></p> <p>Specifically, we begin by having both models produce a response that follows a certain generation configuration. The <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence" rel="external nofollow noopener" target="_blank">Kullback-Leibler divergence</a> distance between the distributions of the two models is then calculated.</p> \[D_{KL}(\pi_{PPO}(y | x) || \pi_{base}(y | x))\] <p><em>with \(\pi_{PPO}\) and \(\pi_{base}\) denoted the respective weights of the models.</em></p> <p>In parallel with this process, the reward \(r\_{\theta} (y \vert x)\) given by the reward model is calculated, which is added to the penalty given by the previous step. At this point it is the job of the <a href="https://openai.com/research/openai-baselines-ppo" rel="external nofollow noopener" target="_blank">PPO optimization</a> algorithm to update the tuned model weights based on what it received as input from the previously calculated loss.</p> <h5 id="toxicity-meter-an-easy-way-to-measure-lms-toxicity"> <code class="language-plaintext highlighter-rouge">Toxicity Meter</code>: an easy way to measure LMs toxicity</h5> <p>Also provided in the <a href="https://github.com/DanielSc4/RewardLM" rel="external nofollow noopener" target="_blank">🥞 RewardLM</a> library is a tool for measuring the average toxicity of models, ⚖️ <code class="language-plaintext highlighter-rouge">Toxicity Meter</code>. By default, the tool employs the <code class="language-plaintext highlighter-rouge">RealToxicityPrompts</code> dataset <a href="https://aclanthology.org/2020.findings-emnlp.301/" rel="external nofollow noopener" target="_blank">(Gehman et al., 2020)</a>. It was therefore possible to quantitatively measure not only the initial toxicity of the different models, but also the post fine-tuning toxicity and RLAF. The toxicity itself can be measured either from any of the model configuration(s) used as reward model for RLAF, or from <a href="https://perspectiveapi.com/" rel="external nofollow noopener" target="_blank">Perspective API</a>, offering a better granularity in the different types of toxicity.</p> <p><br></p> <h2 id="-experiment-and-results"><strong>🔬 Experiment and results</strong></h2> <p>As mentioned generative models are chosen to carry out the first experiments. Among the models selected by the HuggingFace Hub are <code class="language-plaintext highlighter-rouge">togethercomputer/RedPajama-INCITE-Chat-3B-v1</code> (<a href="https://huggingface.co/togethercomputer/RedPajama-INCITE-Chat-3B-v1" rel="external nofollow noopener" target="_blank">🤗 Hub ref.</a>) and <code class="language-plaintext highlighter-rouge">tiiuae/falcon-7b-instruct</code> (<a href="https://huggingface.co/tiiuae/falcon-7b-instruct" rel="external nofollow noopener" target="_blank">🤗 Hub ref.</a>), with 3 and 7 billion parameters, respectively. Their chat/istructed version was chosen to retain the ability to use their conversational capabilities, similar to what has been observed with the more popular <a href="https://openai.com/blog/chatgpt" rel="external nofollow noopener" target="_blank">ChatGPT</a> from OpenAI and <a href="https://bard.google.com" rel="external nofollow noopener" target="_blank">BARD</a> from Google.</p> <h3 id="result">Result</h3> <p>Results are calculated with the toxicity level reported by ⚖️ <code class="language-plaintext highlighter-rouge">Toxicity Meter</code>. They are further broken down into two tables highlighted below, where first all prompts from RealToxicityPrompts are present and then only those considered as toxic by the reward model itself.</p> <table> <thead> <tr> <th>Toxicity, all prompts</th> <th style="text-align: center">PT (Baseline)</th> <th style="text-align: center">Fine-tuned</th> <th style="text-align: center">RLAF</th> </tr> </thead> <tbody> <tr> <td>RedPajama-INCITE-Chat-3B</td> <td style="text-align: center">0.130</td> <td style="text-align: center"><strong>0.092</strong></td> <td style="text-align: center">0.099</td> </tr> <tr> <td>falcon-7b-instruct</td> <td style="text-align: center">0.095</td> <td style="text-align: center"><strong>0.078</strong></td> <td style="text-align: center">0.082</td> </tr> </tbody> </table> <table> <thead> <tr> <th>Toxicity, only toxic prompts</th> <th style="text-align: center">PT (Baseline)</th> <th style="text-align: center">Fine-tuned</th> <th style="text-align: center">RLAF</th> </tr> </thead> <tbody> <tr> <td>RedPajama-INCITE-Chat-3B</td> <td style="text-align: center">0.217</td> <td style="text-align: center"><strong>0.129</strong></td> <td style="text-align: center">0.160</td> </tr> <tr> <td>falcon-7b-instruct</td> <td style="text-align: center">0.140</td> <td style="text-align: center"><strong>0.107</strong></td> <td style="text-align: center">0.125</td> </tr> </tbody> </table> <p><em>Toxicity level, lower is better. <code class="language-plaintext highlighter-rouge">PT</code> stands for pre-trained model, aka the model after its pretraining and instruct fine-tuning phase (as described in the original paper from each model)</em></p> <p>The results obtained show that even <strong>without any limitation</strong> imposed on the models, a <strong>~30% reduction in toxicity is observed for the <code class="language-plaintext highlighter-rouge">RedPajama</code> fine-tuned model</strong> (~20% for <code class="language-plaintext highlighter-rouge">falcon</code> model) and ~24% for the model with RLAF one (~14% for <code class="language-plaintext highlighter-rouge">falcon</code>). <strong>The results improve when considering only the most toxic prompts, with a ~40% reduction for the <code class="language-plaintext highlighter-rouge">RedPajama</code> fine-tuned model</strong> (28% for <code class="language-plaintext highlighter-rouge">falcon</code>) and ~26% for the model with RLAF (~11% for <code class="language-plaintext highlighter-rouge">falcon</code>).</p> <p>It can be seen from the following flowcharts how the toxic responses shifted to contents considered less toxic by <a href="https://perspectiveapi.com/" rel="external nofollow noopener" target="_blank">Perspective API</a>; the different toxicity buckets are assigned as low ( \(x &lt; 0.33\) ), medium ( \(0.33 \leq x \leq 0.66\) ) and high ( \(x &gt; 0.66\) ):</p> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/detox_LMs/sankeymatic.png" sizes="95vw"></source> <img src="/assets/img/detox_LMs/sankeymatic.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="lazy" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"> </picture> </figure> <p><em>Flow chart highlighting the shifts for different responses from <code class="language-plaintext highlighter-rouge">RedPajama</code> model. Starting from the center with the pre-trained (<code class="language-plaintext highlighter-rouge">PT</code>), model’ responses moves to left for the fine-tuned (<code class="language-plaintext highlighter-rouge">FT</code>) and right for the model trained with Reinforcement Learning (<code class="language-plaintext highlighter-rouge">RL</code>).</em></p> <p><br></p> <h2 id="-current-status-and-new-research-questions"><strong>🚀 Current status and new research questions</strong></h2> <blockquote> <p>Editor Note: The following chapter contains content that is mainly derived from assumptions about future directions. None of this represents a constraint or formal expression of the work’s intentions.</p> </blockquote> <p>Considering the pipeline for training through fine-tuning and reinforcement learning, it will be intriguing to be able to extend the research to models of larger dimensions. Specifically, through the use of 4-bit mode, it is possible to scale up the number of parameters, being able to observe the behaviour of larger and more accurate models as well as with more “reasoning” capabilities.</p> <p>Possible future work with the models now available could be to perform a sanity check following the pre-trained model response generation. The token level log probability of the fine-tuned model or RLAF model is expected to be lower if compared to its pre-trained version. Moreover, from an interpretability point of view, measures such as entropy could be exploited to measure the <em>uncertainty</em> of the model producing a response; if the per token attribution entropy of the fine-tuned/RLAF model turn out to be much higher if compared to the pre-trained one the further trained model is not relying on the prompt anymore, suggesting a strange behaviour to be further analyzed.</p> <p>Other analysis can be made on RL models and how their responses relate with the prompt, comparing the output with the fine-tuned model: following the reward model, the LMs should avoid toxic responses, but does it still generate meaningful responses? Is it trying to avoid the penalty generating random text? Could be employed semantic similarity in this manner, trying to identify the connection between prompt and response itself.</p> </div> </article> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2024 Daniel Scalena. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-publications",title:"publications",description:"publications by categories in reversed chronological order",section:"Navigation",handler:()=>{window.location.href="/publications/"}},{id:"nav-cv",title:"cv",description:"This is a description of the page. You can modify it in &#39;_pages/cv.md&#39;. You can also change or remove the top pdf download button.",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"post-in-depth-notes-to-understand-more-about-transformers",title:"In-depth Notes to understand more about Transformers",description:"Start of the truly in-depth study of the state-of-the-art for Mechanistic Interpretability",section:"Posts",handler:()=>{window.location.href="/blog/2023/transformers-notes/"}},{id:"post-let-the-models-respond-interpreting-the-detoxification-process-of-lms",title:"Let the Models Respond: Interpreting the Detoxification process of LMs",description:"Journal to keep track of work during internship @RUG",section:"Posts",handler:()=>{window.location.href="/blog/2023/interpreting-detox-LM/"}},{id:"news-graduated-thesis-here",title:"Graduated! Thesis here \ud83c\udf93",description:"",section:"News"},{id:"news-presenting-my-poster-and-paper-https-arxiv-org-abs-2309-00751-at-the-blackboxnlp-workshop-emnlp-2023-https-2023-emnlp-org-in-singapore",title:"Presenting my poster and [paper](https://arxiv.org/abs/2309.00751) at the BlackBoxNLP workshop [@EMNLP 2023](https://2023.emnlp.org) in Singapore...",description:"",section:"News"},{id:"news-new-work-available-on-arxiv-multi-property-steering-of-large-language-models-with-dynamic-activation-composition-https-arxiv-org-abs-2406-17563",title:"\ud83d\udcda New work available on arXiv: [Multi-property Steering of Large Language Models with...",description:"",section:"News"},{id:"news-multi-property-steering-paper-https-arxiv-org-abs-2406-17563-accepted-to-blackboxnlp-2024-emnlp-2024-and-a-gentle-push-funziona-benissimo-https-www-danielsc4-it-assets-pdf-italian-steering-clic-it-2024-pdf-accepted-clic-it-conference",title:"\ud83d\udcdc [Multi-property Steering paper](https://arxiv.org/abs/2406.17563) accepted to BlackBoxNLP 2024 (@EMNLP 2024) and \ud83d\udcdc [A...",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%73%63%61%6C%65%6E%61%39%39@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-google-scholar",title:"Google Scholar",section:"Socials",handler:()=>{window.open("https://scholar.google.com/citations?user=5Q89Yd8AAAAJ","_blank")}},{id:"socials-semantic-scholar",title:"Semantic Scholar",section:"Socials",handler:()=>{window.open("https://www.semanticscholar.org/author/2221318599","_blank")}},{id:"socials-github",title:"GitHub",section:"Socials",handler:()=>{window.open("https://github.com/danielsc4","_blank")}},{id:"socials-linkedin",title:"LinkedIn",section:"Socials",handler:()=>{window.open("https://www.linkedin.com/in/daniel-scalena","_blank")}},{id:"socials-x",title:"X",description:"Twitter",section:"Socials",handler:()=>{window.open("https://twitter.com/daniel_sc4","_blank")}},{id:"socials-bluesky",title:"Bluesky",section:"Socials",handler:()=>{window.open("https://bsky.app/profile/danielsc4.it","_blank")}},{id:"socials-discord",title:"Discord",section:"Socials",handler:()=>{window.open("https://discord.com/users/709147478963781600","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>