{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/daniel/Documents/Work/research/general-task-vectors/.venv/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prompt management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (\n",
    "    ('Rome', 'Italy'),\n",
    "    ('Madrid', 'Spain'),\n",
    "    ('Amsterdam', 'Netherlands'),\n",
    "    ('Moscow', 'Russia'),\n",
    "    ('Nairobi', 'Kenya'),\n",
    "    ('Paris', 'France'),\n",
    "    ('Tehran', 'Iran'),\n",
    "    ('Tokyo', 'Japan'),\n",
    "    ('Warsaw', 'Poland'),\n",
    "    ('Ottawa', 'Canada'),\n",
    "    ('Oslo', 'Norway'),\n",
    "    ('Lisbon', 'Portugal'),\n",
    "    ('Helsinki', 'Finland'),\n",
    "    ('Havana', 'Cuba'),\n",
    "    ('Doha', 'Qatar'),\n",
    "    ('Damascus', 'Syria'),\n",
    "    ('Canberra', 'Australia'),\n",
    "    ('Cairo', 'Egypt'),\n",
    "    ('Bern', 'Switzerland'),\n",
    "    ('Berlin', 'Germany'),\n",
    "    ('Beijing', 'China'),\n",
    "    ('Athens', 'Greece'),\n",
    "    ('Ankara', 'Turkey'),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from src.utils.model_utils import load_gpt_model_and_tokenizer, set_seed, get_submodule\n",
    "from src.extraction import split_activation\n",
    "from src.utils.prompt_helper import build_prompt_txt, tokenize_from_template, tokenize_ICL\n",
    "set_seed(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2LMHeadModel(\n",
      "  (transformer): GPT2Model(\n",
      "    (wte): Embedding(50257, 768)\n",
      "    (wpe): Embedding(1024, 768)\n",
      "    (drop): Dropout(p=0.1, inplace=False)\n",
      "    (h): ModuleList(\n",
      "      (0-11): 12 x GPT2Block(\n",
      "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (attn): GPT2AttentionAltered(\n",
      "          (c_attn): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "          (query): WrapperModule()\n",
      "          (key): WrapperModule()\n",
      "          (value): WrapperModule()\n",
      "        )\n",
      "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        (mlp): GPT2MLP(\n",
      "          (c_fc): Conv1D()\n",
      "          (c_proj): Conv1D()\n",
      "          (act): NewGELUActivation()\n",
      "          (dropout): Dropout(p=0.1, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
      ")\n",
      "{'n_heads': 12, 'n_layer': 12, 'd_model': 768, 'name': 'gpt2', 'layer_name': 'transformer.h', 'layer_hook_names': ['transformer.h.0', 'transformer.h.1', 'transformer.h.2', 'transformer.h.3', 'transformer.h.4', 'transformer.h.5', 'transformer.h.6', 'transformer.h.7', 'transformer.h.8', 'transformer.h.9', 'transformer.h.10', 'transformer.h.11'], 'attn_name': 'attn.c_proj', 'attn_hook_names': ['transformer.h.0.attn.c_proj', 'transformer.h.1.attn.c_proj', 'transformer.h.2.attn.c_proj', 'transformer.h.3.attn.c_proj', 'transformer.h.4.attn.c_proj', 'transformer.h.5.attn.c_proj', 'transformer.h.6.attn.c_proj', 'transformer.h.7.attn.c_proj', 'transformer.h.8.attn.c_proj', 'transformer.h.9.attn.c_proj', 'transformer.h.10.attn.c_proj', 'transformer.h.11.attn.c_proj']}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model, config = load_gpt_model_and_tokenizer(model_name)\n",
    "print(model)\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the average of the activation on the dataset to grasp any pattern that is task-dependant and not token-dependant \n",
    "\n",
    "Activations have the following shape (num_layers, num_heads, seq_len, d_head), being each OV circuit output.\n",
    "\n",
    "Still to answer:\n",
    "- How this is connected to the residual stream?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying to use [nnsight](https://github.com/JadenFiotto-Kaufman/nnsight) and explore to know how it works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Q:', 'structural'),\n",
       " ('one one one', 'sentence'),\n",
       " ('\\nA:', 'structural'),\n",
       " ('two', 'sentence'),\n",
       " ('\\n\\n', 'structural'),\n",
       " ('Q:', 'structural'),\n",
       " ('three three three', 'sentence'),\n",
       " ('\\nA:', 'structural'),\n",
       " ('four', 'sentence'),\n",
       " ('\\n\\n', 'structural'),\n",
       " ('Q:', 'structural'),\n",
       " ('five five five', 'sentence'),\n",
       " ('\\nA:', 'structural')]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# example prompt\n",
    "promtp_w_template = [\n",
    "    ('Q', 'structural'),\n",
    "    (':', 'structural'),\n",
    "    ('Hello my name is daniel', 'sentence'),\n",
    "    ('\\n', 'structural'),\n",
    "    ('A', 'structural'),\n",
    "    (':', 'structural'),\n",
    "    ('This is a response', 'sentence'),\n",
    "]\n",
    "\n",
    "\n",
    "prompt2 = build_prompt_txt(\n",
    "    queries=['one one one', 'three three three', 'five five five'], answers=['two', 'four']\n",
    ")\n",
    "prompt2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([50256,    48,    25,   505,   530,   530,   198,    32,    25, 11545,\n",
      "          628,    48,    25, 15542,  1115,  1115,   198,    32,    25, 14337,\n",
      "          628,    48,    25, 13261,  1936,  1936,   198,    32,    25])\n",
      "[0, 1, 2, 5, 6, 7, 8, 9, 10, 11, 12, 15, 16, 17, 18, 19, 20, 21, 22, 25, 26, 27, 28]\n"
     ]
    }
   ],
   "source": [
    "# prompt template:\n",
    "# tutti gli indici dei token structural vanno salvati, solo l'ultimo indice dei token sentence vanno salvati\n",
    "\n",
    "\n",
    "\n",
    "full_tokenized, indexes = tokenize_from_template(tokenizer=tokenizer, promtp_w_template=prompt2)\n",
    "print(full_tokenized)\n",
    "print(indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# create template based on ICL_examples\n",
    "\n",
    "# params: dataset, ICL_examples\n",
    "\n",
    "# create template\n",
    "# split\n",
    "# store tokenized and indexes from tokenize_from_template\n",
    "\n",
    "# select number of ICL examples\n",
    "ICL_examples = 2\n",
    "\n",
    "\n",
    "tok_ret, ids_ret = tokenize_ICL(tokenizer, ICL_examples = 2, dataset = dataset)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/daniel/Documents/Work/research/general-task-vectors/notebooks/exploration.ipynb Cell 11\u001b[0m line \u001b[0;36m3\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/daniel/Documents/Work/research/general-task-vectors/notebooks/exploration.ipynb#W5sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m         dataset_activations\u001b[39m.\u001b[39mappend(attn_activations)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/daniel/Documents/Work/research/general-task-vectors/notebooks/exploration.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m dataset_activations, outputs\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/daniel/Documents/Work/research/general-task-vectors/notebooks/exploration.ipynb#W5sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m activations, outputs \u001b[39m=\u001b[39m extract_activations(tot_prompts\u001b[39m=\u001b[39;49mtot_prompts, model\u001b[39m=\u001b[39;49mmodel, config\u001b[39m=\u001b[39;49mconfig)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/daniel/Documents/Work/research/general-task-vectors/notebooks/exploration.ipynb#W5sZmlsZQ%3D%3D?line=31'>32</a>\u001b[0m activations[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mshape\n",
      "\u001b[1;32m/Users/daniel/Documents/Work/research/general-task-vectors/notebooks/exploration.ipynb Cell 11\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/daniel/Documents/Work/research/general-task-vectors/notebooks/exploration.ipynb#W5sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m         layer_attn_activations \u001b[39m=\u001b[39m []\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/daniel/Documents/Work/research/general-task-vectors/notebooks/exploration.ipynb#W5sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m         \u001b[39mfor\u001b[39;00m layer \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(config[\u001b[39m'\u001b[39m\u001b[39mn_layer\u001b[39m\u001b[39m'\u001b[39m]):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/daniel/Documents/Work/research/general-task-vectors/notebooks/exploration.ipynb#W5sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m             layer_attn_activations\u001b[39m.\u001b[39mappend(model\u001b[39m.\u001b[39mtransformer\u001b[39m.\u001b[39mh[layer]\u001b[39m.\u001b[39mattn\u001b[39m.\u001b[39mc_proj\u001b[39m.\u001b[39moutput\u001b[39m.\u001b[39msave())\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/daniel/Documents/Work/research/general-task-vectors/notebooks/exploration.ipynb#W5sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m outputs\u001b[39m.\u001b[39mappend(generator\u001b[39m.\u001b[39moutput)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/daniel/Documents/Work/research/general-task-vectors/notebooks/exploration.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# get the values\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/Work/research/general-task-vectors/.venv/lib/python3.9/site-packages/nnsight/contexts/Runner.py:67\u001b[0m, in \u001b[0;36mRunner.__exit__\u001b[0;34m(self, exc_type, exc_val, exc_tb)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrun_server()\n\u001b[1;32m     66\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 67\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrun_local()\n",
      "File \u001b[0;32m~/Documents/Work/research/general-task-vectors/.venv/lib/python3.9/site-packages/nnsight/contexts/Runner.py:72\u001b[0m, in \u001b[0;36mRunner.run_local\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Runs the local_model using it's chosen method.\"\"\"\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39m# Run the model and store the output.\u001b[39;00m\n\u001b[0;32m---> 72\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m     73\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49m_generation \u001b[39mif\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgeneration \u001b[39melse\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49m_forward,\n\u001b[1;32m     74\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbatched_input,\n\u001b[1;32m     75\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgraph,\n\u001b[1;32m     76\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49margs,\n\u001b[1;32m     77\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs,\n\u001b[1;32m     78\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/Work/research/general-task-vectors/.venv/lib/python3.9/site-packages/nnsight/models/AbstractModel.py:206\u001b[0m, in \u001b[0;36mAbstractModel.__call__\u001b[0;34m(self, fn, inputs, graph, edits, inference, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m     increment_hook\u001b[39m.\u001b[39mremove()\n\u001b[1;32m    204\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCompleted `\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrepoid_path_clsname\u001b[39m}\u001b[39;00m\u001b[39m`\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 206\u001b[0m     gc\u001b[39m.\u001b[39;49mcollect()\n\u001b[1;32m    207\u001b[0m     torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mempty_cache()\n\u001b[1;32m    209\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# some queries k=1 ICL\n",
    "\n",
    "template = 'Q:{query}\\nA:{response}\\n'\n",
    "\n",
    "tot_prompts = [\n",
    "    template.format(query = query, response = response)\n",
    "    for query, response in dataset\n",
    "]\n",
    "\n",
    "AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "def extract_activations(tot_prompts, model, config):\n",
    "    dataset_activations, outputs = [], []\n",
    "    for prompt in tot_prompts:\n",
    "        with model.generate(max_new_tokens=3) as generator:\n",
    "            # invoke works in a generation context, where operations on inputs and outputs are tracked\n",
    "            with generator.invoke(prompt) as invoker:\n",
    "                layer_attn_activations = []\n",
    "                for layer in range(config['n_layer']):\n",
    "                    layer_attn_activations.append(model.transformer.h[layer].attn.c_proj.output.save())\n",
    "        outputs.append(generator.output)\n",
    "\n",
    "\n",
    "        # get the values\n",
    "        layer_attn_activations = [att.value for att in layer_attn_activations]\n",
    "        attn_activations = split_activation(layer_attn_activations, config).permute(0, 2, 1, 3)     # n_layers, tokens, n_heads, d_head -> n_layers, n_heads, tokens, d_head\n",
    "        dataset_activations.append(attn_activations)\n",
    "    return dataset_activations, outputs\n",
    "\n",
    "activations, outputs = extract_activations(tot_prompts=tot_prompts, model=model, config=config)\n",
    "activations[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Paris: France, Berlin: Germany, Paris'"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode([40313,    25,  4881,    11, 11307,    25,  4486,    11,  6342])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([12, 12, 6, 64])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "activations[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "calculate mean activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "p( Spain) = 0.900\n",
      "p( Portugal) = 0.028\n",
      "p( Madrid) = 0.024\n",
      "p( Italy) = 0.014\n",
      "p( Argentina) = 0.005\n",
      "p( Mexico) = 0.003\n",
      "p( France) = 0.002\n",
      "p( Uruguay) = 0.002\n",
      "p( Brazil) = 0.002\n",
      "p( Real) = 0.001\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Rome: Italy, Amsterdam: Netherlands, Lisbon: Portugal, Madrid:\"\n",
    "\n",
    "with model.invoke(prompt) as invoker:\n",
    "    pass # no changes to make in the forward pass\n",
    "logits = invoker.output.logits\n",
    "logits = logits[0,-1,:] # only over the final token\n",
    "probs = logits.softmax(dim=-1)\n",
    "\n",
    "\n",
    "for tok, prob in zip(probs.topk(10).indices, probs.topk(10).values):\n",
    "    print(f\"p({model.tokenizer.decode(tok)}) = {prob:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
