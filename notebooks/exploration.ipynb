{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'gpt2'\n",
    "MODEL_NAME = 'microsoft/phi-2'\n",
    "# MODEL_NAME = 'EleutherAI/pythia-1B'\n",
    "# MODEL_NAME = 'TinyLlama/TinyLlama-1.1B-Chat-v1.0'\n",
    "\n",
    "LOAD_IN_8BIT = False\n",
    "RELATIVE_PATH = '../'\n",
    "\n",
    "dataset_name = 'country-capital'\n",
    "# select number of ICL examples (query excluded)\n",
    "ICL_examples = 4\n",
    "debug = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset len: 50\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def load_json_dataset(json_path):\n",
    "    with open(json_path, encoding='utf-8') as file:\n",
    "        dataset = json.load(file)\n",
    "    return dataset\n",
    "\n",
    "dataset = load_json_dataset(f'{RELATIVE_PATH}data/{dataset_name}.json')\n",
    "dataset = list(map(lambda x: tuple(x.values()), dataset))\n",
    "\n",
    "if debug:\n",
    "    dataset = dataset[0:50]\n",
    "\n",
    "print(f'dataset len: {len(dataset)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "torch.set_grad_enabled(False)\n",
    "\n",
    "from src.utils.model_utils import load_gpt_model_and_tokenizer, set_seed\n",
    "from src.extraction import get_mean_activations\n",
    "from src.utils.prompt_helper import tokenize_ICL\n",
    "from src.intervention import compute_indirect_effect\n",
    "set_seed(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "model, tokenizer, config, device = load_gpt_model_and_tokenizer(MODEL_NAME, LOAD_IN_8BIT)\n",
    "tok_ret, ids_ret, correct_labels = tokenize_ICL(tokenizer, ICL_examples = ICL_examples, dataset = dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get activations and measure head's importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6af27fbe5e94476ae94dec66fb5ec45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a CodeGenTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-0.7236,  0.7874,  0.1998,  ..., -0.1322,  0.7554, -0.5354],\n",
      "         [-0.5455,  0.9391,  0.2884,  ..., -0.1260,  0.2966, -0.1629]]],\n",
      "       device='cuda:0'), None, <transformers.cache_utils.DynamicCache object at 0x7f72924e00a0>)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from nnsight import LanguageModel\n",
    "\n",
    "model = LanguageModel(\"microsoft/phi-2\", trust_remote_code = True, device_map = 'auto')\n",
    "\n",
    "layer_attn_activations =[]\n",
    "\n",
    "with model.generate(max_new_tokens=1) as runner:\n",
    "\n",
    "    with runner.invoke(\"blah\"):\n",
    "\n",
    "        vv0 = model.model.layers[0].self_attn.output.save()\n",
    "        vv1 = model.model.layers[1].self_attn.output.save()\n",
    "        vv2 = model.model.layers[2].self_attn.output.save()\n",
    "\n",
    "        for layer_num in range(len(model.model.layers)):\n",
    "            layer_attn_activations.append(\n",
    "                model.model.layers[layer_num].self_attn.output.save()\n",
    "            )\n",
    "\n",
    "print(layer_attn_activations[0].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-0.0508, -0.3839,  0.9552,  ..., -0.7086,  0.4718, -0.1021],\n",
      "         [-0.0082, -0.0214,  0.5937,  ..., -0.3852,  0.3731, -0.0517],\n",
      "         [-0.4863, -0.0433,  0.7666,  ..., -0.1964,  0.2062,  0.1382],\n",
      "         ...,\n",
      "         [ 0.1964,  0.0670, -0.0347,  ..., -0.0307,  0.1573,  0.1318],\n",
      "         [-0.0805,  0.1059,  0.0777,  ..., -0.0269,  0.0157, -0.1853],\n",
      "         [ 0.1935,  0.0671, -0.0260,  ..., -0.1173,  0.1092, -0.2673]]],\n",
      "       device='cuda:0'), None, <transformers.cache_utils.DynamicCache object at 0x7f97cc0cbbe0>)\n"
     ]
    }
   ],
   "source": [
    "prompt = tok_ret[0]\n",
    "# rgetattr(model, config['attn_hook_names'][0]).output.save()\n",
    "\n",
    "with model.generate(max_new_tokens=1, pad_token_id=tokenizer.pad_token_id) as generator:\n",
    "    prompt = prompt.to(device)\n",
    "    with generator.invoke(prompt) as invoker:\n",
    "        # this is good\n",
    "        vv0 = model.model.layers[0].self_attn.output.save()\n",
    "        vv1 = model.model.layers[1].self_attn.output.save()\n",
    "        vv2 = model.model.layers[2].self_attn.output.save()\n",
    "        vv2 = model.model.layers[31].self_attn.output.save()\n",
    "        # etc\n",
    "\n",
    "        layer_attn_activations = []\n",
    "        # this leads to an error: AttributeError: 'NoneType' object has no attribute 'to_legacy_cache'\n",
    "        for layer_num in range(config['n_layers']):\n",
    "            layer_attn_activations.append(\n",
    "                model.model.layers[layer_num].self_attn.output.save()\n",
    "            )\n",
    "print(vv0.value)\n",
    "            # module_path = modules_pre_layer + [str(layer_num)] + modules_post_layer\n",
    "            # layer_attn_activations.append(\n",
    "            #     functools.reduce(getattr, [model] + module_path).output.save()  # get model.model.layers.<layer_num>.self_attn (ERROR here)\n",
    "            # )\n",
    "            # modules_pre_layer = config['layer_name'].split('.')     # = ['model', 'layers']\n",
    "            # modules_post_layer = config['attn_name'].split('.')     # = ['self_attn']\n",
    "        \n",
    "\n",
    "        # for layer in functools.reduce(getattr, [model] + config['layer_name'].split('.')):\n",
    "        #     layer.\n",
    "        #     attn_activation = functools.reduce(\n",
    "        #         getattr, [layer] + config['attn_name'].split('.')\n",
    "        #     ).output.save()\n",
    "        #     # layer_attn_activations.append(attn_activation)\n",
    "        \n",
    "        # this leads to an error: AttributeError: 'NoneType' object has no attribute 'to_legacy_cache'\n",
    "        # layer_attn_activations = []\n",
    "        # for layer_n in range(3):\n",
    "        #     layer_attn_activations.append(\n",
    "        #         model.model.layers[layer_n].self_attn.output.save()\n",
    "        #     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[x] Extracting activations (layer: 31/32):   0%|          | 0/10 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m mean_activations \u001b[38;5;241m=\u001b[39m \u001b[43mget_mean_activations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenized_prompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtok_ret\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m    \u001b[49m\u001b[43mimportant_ids\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mids_ret\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcorrect_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorrect_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\n\u001b[1;32m      9\u001b[0m \u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m torch\u001b[38;5;241m.\u001b[39msave(mean_activations, \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mRELATIVE_PATH\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124moutput/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_mean_activations_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mMODEL_NAME\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     11\u001b[0m mean_activations\u001b[38;5;241m.\u001b[39mshape\n",
      "File \u001b[0;32m~/document/general-task-vectors/notebooks/../src/extraction.py:97\u001b[0m, in \u001b[0;36mget_mean_activations\u001b[0;34m(tokenized_prompts, important_ids, tokenizer, model, config, correct_labels, device)\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_mean_activations\u001b[39m(\n\u001b[1;32m     74\u001b[0m         tokenized_prompts: \u001b[38;5;28mlist\u001b[39m[torch\u001b[38;5;241m.\u001b[39mTensor], \n\u001b[1;32m     75\u001b[0m         important_ids: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mint\u001b[39m], \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     80\u001b[0m         device: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m     81\u001b[0m     ):\n\u001b[1;32m     82\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute the average of all the model's activation on the provided prompts\u001b[39;00m\n\u001b[1;32m     83\u001b[0m \n\u001b[1;32m     84\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;124;03m        torch.Tensor: mean of activations (`n_layers, n_heads, seq_len, d_head`)\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 97\u001b[0m     activations, outputs \u001b[38;5;241m=\u001b[39m \u001b[43mextract_activations\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenized_prompts\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenized_prompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# keep only important tokens (activations_clean: [batch, n_layers, n_heads, seq, d_head])\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     activations_clean \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack(\n\u001b[1;32m    107\u001b[0m         [activations[i][:, :, important_ids[i], :] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(activations))]\n\u001b[1;32m    108\u001b[0m     )\n",
      "File \u001b[0;32m~/document/general-task-vectors/notebooks/../src/extraction.py:67\u001b[0m, in \u001b[0;36mextract_activations\u001b[0;34m(tokenized_prompts, model, config, tokenizer, device)\u001b[0m\n\u001b[1;32m     64\u001b[0m     outputs\u001b[38;5;241m.\u001b[39mappend(generator\u001b[38;5;241m.\u001b[39moutput)\n\u001b[1;32m     66\u001b[0m     \u001b[38;5;66;03m# from hidden state split heads and permute: n_layers, tokens, n_heads, d_head -> n_layers, n_heads, tokens, d_head\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m     attn_activations \u001b[38;5;241m=\u001b[39m \u001b[43msplit_activation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_attn_activations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m     68\u001b[0m     dataset_activations\u001b[38;5;241m.\u001b[39mappend(attn_activations)\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m dataset_activations, outputs\n",
      "File \u001b[0;32m~/document/general-task-vectors/notebooks/../src/extraction.py:20\u001b[0m, in \u001b[0;36msplit_activation\u001b[0;34m(activations, config)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msplit_activation\u001b[39m(activations, config):\n\u001b[1;32m     10\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"split the residual stream (d_model) into n_heads activations for each layer\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \n\u001b[1;32m     12\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;124;03m        activations: reshaped activation in [n_layers, seq, n_heads, d_head]\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     new_shape \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mSize([\n\u001b[0;32m---> 20\u001b[0m         \u001b[43mactivations\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m[\u001b[38;5;241m0\u001b[39m],         \u001b[38;5;66;03m# batch_size == 1\u001b[39;00m\n\u001b[1;32m     21\u001b[0m         activations[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],         \u001b[38;5;66;03m# seq_len\u001b[39;00m\n\u001b[1;32m     22\u001b[0m         config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_heads\u001b[39m\u001b[38;5;124m'\u001b[39m],                          \u001b[38;5;66;03m# n_heads\u001b[39;00m\n\u001b[1;32m     23\u001b[0m         config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md_model\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_heads\u001b[39m\u001b[38;5;124m'\u001b[39m],     \u001b[38;5;66;03m# d_head\u001b[39;00m\n\u001b[1;32m     24\u001b[0m     ])\n\u001b[1;32m     25\u001b[0m     attn_activations \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mvstack([act\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m*\u001b[39mnew_shape) \u001b[38;5;28;01mfor\u001b[39;00m act \u001b[38;5;129;01min\u001b[39;00m activations])\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_activations\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "mean_activations = get_mean_activations(\n",
    "    tokenized_prompts=tok_ret,\n",
    "    important_ids=ids_ret,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    config=config,\n",
    "    correct_labels=correct_labels,\n",
    "    device='cuda'\n",
    ")\n",
    "torch.save(mean_activations, f'{RELATIVE_PATH}output/{dataset_name}_mean_activations_{MODEL_NAME.replace(\"/\", \"-\")}.pt')\n",
    "mean_activations.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cie, probs_original, probs_edited  = compute_indirect_effect(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    config=config,\n",
    "    dataset=dataset, \n",
    "    mean_activations=mean_activations,\n",
    "    ICL_examples = ICL_examples,\n",
    "    batch_size=15,\n",
    ")\n",
    "torch.save(cie, f'{RELATIVE_PATH}output/{dataset_name}_cie_{MODEL_NAME.replace(\"/\", \"-\")}.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cie = torch.load(f'{RELATIVE_PATH}output/sentiment_cie_gpt2.pt')\n",
    "\n",
    "import plotly.express as px\n",
    "\n",
    "fig = px.imshow(cie.mean(dim=0))\n",
    "fig.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
